Relational Reinforcement Learning - RRL

Reinforcement Learning:
An Introduction
http://www.cs.ualberta.ca/~sutton/book/the-book.html

====>>>>> 9.3 When the Model Is Wrong

A Tutorial on Dynamic Programming
http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html

Resources on Reinforcement Learning - Book, FAQ
http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/Resources.html

Pattern recognition: context-dependent classification
http://www.it.lut.fi/kurssit/02-03/010586001/lectures/lecture12.pdf

=====================
ОПИСАНИЕ ГЛОБАЛЬНОЙ СТРАТЕГИИ:

Вся сложность бытия образуется из алгоритмической работы по соединению
обращённого дерева подцелей (обращенного потому что оно растёт от глобальной
возможно бесконечно удалённой цели к нам) и дерева предсказаний, растущего
от нас, и описывающего наши текущие изученные и осознанные возможности

*) "то что ещё не пробовали" это хороший принцип, надо его юзать,
только создать индекс в истории - и это станет быстрой операцией
*) до первого result(i)>0 работаем по плану
 - если не всё предсказано, выбираем из команд с неполными предсказаниями то,
   что "ещё не пробовали"
 - если всё предсказано, выбираем из всех команд
   используя принцип "ещё не пробовали"
*) цель бытия нашего существа всё-таки не обучение, а результат.
   Обучение по-видимому является limit цепочки подцелей в обращённом дереве
   подцелей - подцели по мере углубления в дерево всё более отдаляются
   и становятся не похожими на нашу глобальную цель, и их всё более глубокий
   анализ для построения ещё более далёких веток в дереве приводит к такой
   подцели как "обучение"
*) когда встретился первый result(i)>0 - работаем на обучение/достижение вокруг
   него. Пытаемся подобрать "условие -> наш результат" перебором элементов из
   view. Сейчас пока цель - это только "result(any i)>0". Когда условие у1,
   дающее по известному пути (известной команде) нашу цель result(i)>0
   найдено, появляются подцели - при невозможности непосредственного достижения
   главной цели, создать хотя бы у1. Стремимся с нашей цели или подцели у1,
   находим у11 - подцель, запорука у1. Убирая часть условий из у1 или у11,
   например у1 == (f=ORANGE), у11 == (f=YELLOW), у111 == (ff=YELLOW, f=WHITE),
   получаем цели со сложно достигаемыми переходами от них в направлении главной
   цели - у1111 == (ff=YELLOW) - может быть достигнута, потом идёт исследование
   перебором на предмет таки достижения у11, без или с попутным достижением у111,
   и тут могут быть весьма замысловатые пути.

   И так дерево подцелей растёт в обратную сторону от prediction tree.
   Дерево подцелей - статическое, мы его
   растим и помним в процессе жизни. Prediction tree динамическое, оно меняется
   на каждом шаге, а растим и помним в процессе жизни мы законы, условия (тоже
   в древовидной форме), по которым это дерево строится. 

=====================
